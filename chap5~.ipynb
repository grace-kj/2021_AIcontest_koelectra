{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chap5~.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1FiEsNkaHzoJ6_09ytmR6LTmvUcIdtsuQ","authorship_tag":"ABX9TyO2NJbgaCwDSUHlF5VnNpUf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"PL0o_iwR_7QR"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBwa35V6ABSF","executionInfo":{"status":"ok","timestamp":1632825817006,"user_tz":-540,"elapsed":2828,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["import os\n","from argparse import Namespace\n","from collections import Counter\n","import json\n","import re\n","import string\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm_notebook"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZIoHRLQAKq0","executionInfo":{"status":"ok","timestamp":1632825959760,"user_tz":-540,"elapsed":374,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["###############벡터로 변환\n","class Vocabulary(object):\n","   \n","   #토큰-인덱스 매핑\n","    def __init__(self, token_to_idx=None):\n","        \n","\n","        if token_to_idx is None:\n","            token_to_idx = {}\n","        self._token_to_idx = token_to_idx\n","\n","        self._idx_to_token = {idx: token \n","                              for token, idx in self._token_to_idx.items()}\n","        \n","    def to_serializable(self):\n","        return {'token_to_idx': self._token_to_idx}\n","\n","    @classmethod\n","    def from_serializable(cls, contents):\n","        return cls(**contents)\n","\n","    def add_token(self, token):\n","        if token in self._token_to_idx:\n","            index = self._token_to_idx[token]\n","        else:\n","            index = len(self._token_to_idx)\n","            self._token_to_idx[token] = index\n","            self._idx_to_token[index] = token\n","        return index\n","            \n","    def add_many(self, tokens):\n","        return [self.add_token(token) for token in tokens]\n","\n","    def lookup_token(self, token):\n","        return self._token_to_idx[token]\n","\n","    def lookup_index(self, index):\n","        if index not in self._idx_to_token:\n","            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n","        return self._idx_to_token[index]\n","\n","    def __str__(self):\n","        return \"<Vocabulary(size=%d)>\" % len(self)\n","\n","    def __len__(self):\n","        return len(self._token_to_idx)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vQLH7FxAuHW","executionInfo":{"status":"ok","timestamp":1632825998847,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["########토큰에 대응하는 인덱스 추출, sequence\n","class SequenceVocabulary(Vocabulary):\n","    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n","                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n","                 end_seq_token=\"<END>\"):\n","\n","        super(SequenceVocabulary, self).__init__(token_to_idx)\n","\n","        self._mask_token = mask_token\n","        self._unk_token = unk_token\n","        self._begin_seq_token = begin_seq_token\n","        self._end_seq_token = end_seq_token\n","\n","        self.mask_index = self.add_token(self._mask_token)\n","        self.unk_index = self.add_token(self._unk_token)\n","        self.begin_seq_index = self.add_token(self._begin_seq_token)\n","        self.end_seq_index = self.add_token(self._end_seq_token)\n","\n","    def to_serializable(self):\n","        contents = super(SequenceVocabulary, self).to_serializable()\n","        contents.update({'unk_token': self._unk_token,\n","                         'mask_token': self._mask_token,\n","                         'begin_seq_token': self._begin_seq_token,\n","                         'end_seq_token': self._end_seq_token})\n","        return contents\n","\n","    def lookup_token(self, token):\n","  \n","        if self.unk_index >= 0:\n","            return self._token_to_idx.get(token, self.unk_index)\n","        else:\n","            return self._token_to_idx[token]"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBTAOqbtA3qU","executionInfo":{"status":"ok","timestamp":1632826054051,"user_tz":-540,"elapsed":376,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["##########vectorizer\n","\n","class NewsVectorizer(object):\n","    def __init__(self, title_vocab, category_vocab):\n","        self.title_vocab = title_vocab\n","        self.category_vocab = category_vocab\n","\n","    def vectorize(self, title, vector_length=-1):\n","        indices = [self.title_vocab.begin_seq_index]\n","        indices.extend(self.title_vocab.lookup_token(token) \n","                       for token in title.split(\" \"))\n","        indices.append(self.title_vocab.end_seq_index)\n","\n","        if vector_length < 0:\n","            vector_length = len(indices)\n","\n","        out_vector = np.zeros(vector_length, dtype=np.int64)\n","        out_vector[:len(indices)] = indices\n","        out_vector[len(indices):] = self.title_vocab.mask_index\n","\n","        return out_vector\n","\n","    @classmethod\n","    #데이터프레임에서 vectorizer 객체 만들기\n","    def from_dataframe(cls, news_df, cutoff=25):\n","        category_vocab = Vocabulary()        \n","        for category in sorted(set(news_df.category)):\n","            category_vocab.add_token(category)\n","\n","        word_counts = Counter()\n","        for title in news_df.title:\n","            for token in title.split(\" \"):\n","                if token not in string.punctuation:\n","                    word_counts[token] += 1\n","        \n","        title_vocab = SequenceVocabulary()\n","        for word, word_count in word_counts.items():\n","            if word_count >= cutoff:\n","                title_vocab.add_token(word)\n","        \n","        return cls(title_vocab, category_vocab)\n","\n","    @classmethod\n","    def from_serializable(cls, contents):\n","        title_vocab = \\\n","            SequenceVocabulary.from_serializable(contents['title_vocab'])\n","        category_vocab =  \\\n","            Vocabulary.from_serializable(contents['category_vocab'])\n","\n","        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n","\n","    def to_serializable(self):\n","        return {'title_vocab': self.title_vocab.to_serializable(),\n","                'category_vocab': self.category_vocab.to_serializable()}"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrCc0GCTBFH9","executionInfo":{"status":"ok","timestamp":1632826180416,"user_tz":-540,"elapsed":389,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["###########데이터셋\n","class NewsDataset(Dataset):\n","    def __init__(self, news_df, vectorizer):\n","        self.news_df = news_df\n","        self._vectorizer = vectorizer\n","\n","        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n","        measure_len = lambda context: len(context.split(\" \"))\n","        self._max_seq_length = max(map(measure_len, news_df.title)) + 2\n","        \n","\n","        self.train_df = self.news_df[self.news_df.split=='train']\n","        self.train_size = len(self.train_df)\n","\n","        self.val_df = self.news_df[self.news_df.split=='val']\n","        self.validation_size = len(self.val_df)\n","\n","        self.test_df = self.news_df[self.news_df.split=='test']\n","        self.test_size = len(self.test_df)\n","\n","        self._lookup_dict = {'train': (self.train_df, self.train_size),\n","                             'val': (self.val_df, self.validation_size),\n","                             'test': (self.test_df, self.test_size)}\n","\n","        self.set_split('train')\n","\n","        # 클래스 가중치\n","        class_counts = news_df.category.value_counts().to_dict()\n","        def sort_key(item):\n","            return self._vectorizer.category_vocab.lookup_token(item[0])\n","        sorted_counts = sorted(class_counts.items(), key=sort_key)\n","        frequencies = [count for _, count in sorted_counts]\n","        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n","        \n","        \n","    @classmethod\n","    #새로운 vectorizer 만들기\n","    def load_dataset_and_make_vectorizer(cls, news_csv):\n","        news_df = pd.read_csv(news_csv)\n","        train_news_df = news_df[news_df.split=='train']\n","        return cls(news_df, NewsVectorizer.from_dataframe(train_news_df))\n","\n","    @classmethod\n","    #vectorizer 객체 로드\n","    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n","        news_df = pd.read_csv(news_csv)\n","        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n","        return cls(news_csv, vectorizer)\n","\n","    @staticmethod\n","    def load_vectorizer_only(vectorizer_filepath):\n","        with open(vectorizer_filepath) as fp:\n","            return NameVectorizer.from_serializable(json.load(fp))\n","    \n","    #json 형태로 저장\n","    def save_vectorizer(self, vectorizer_filepath):\n","        with open(vectorizer_filepath, \"w\") as fp:\n","            json.dump(self._vectorizer.to_serializable(), fp)\n","\n","    def get_vectorizer(self):\n","        return self._vectorizer\n","\n","    def set_split(self, split=\"train\"):\n","        self._target_split = split\n","        self._target_df, self._target_size = self._lookup_dict[split]\n","\n","    def __len__(self):\n","        return self._target_size\n","\n","    def __getitem__(self, index):\n","\n","        row = self._target_df.iloc[index]\n","\n","        title_vector = \\\n","            self._vectorizer.vectorize(row.title, self._max_seq_length)\n","\n","        category_index = \\\n","            self._vectorizer.category_vocab.lookup_token(row.category)\n","\n","        return {'x_data': title_vector,\n","                'y_target': category_index}\n","\n","    def get_num_batches(self, batch_size):\n","      \n","        return len(self) // batch_size\n","\n","#dataloader 제너레이터 함수\n","def generate_batches(dataset, batch_size, shuffle=True,\n","                     drop_last=True, device=\"cpu\"):\n","    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                            shuffle=shuffle, drop_last=drop_last)\n","\n","    for data_dict in dataloader:\n","        out_data_dict = {}\n","        for name, tensor in data_dict.items():\n","            out_data_dict[name] = data_dict[name].to(device)\n","        yield out_data_dict"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"GavZepYdBj-m","executionInfo":{"status":"ok","timestamp":1632826200336,"user_tz":-540,"elapsed":380,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["#######모델: classifier\n","class NewsClassifier(nn.Module):\n","    def __init__(self, embedding_size, num_embeddings, num_channels, \n","                 hidden_dim, num_classes, dropout_p, \n","                 pretrained_embeddings=None, padding_idx=0):\n","        \"\"\"\n","        매개변수:\n","            embedding_size (int): 임베딩 벡터의 크기\n","            num_embeddings (int): 임베딩 벡터의 개수\n","            num_channels (int): 합성곱 커널 개수\n","            hidden_dim (int): 은닉 차원 크기\n","            num_classes (int): 클래스 개수\n","            dropout_p (float): 드롭아웃 확률\n","            pretrained_embeddings (numpy.array): 사전에 훈련된 단어 임베딩\n","                기본값은 None \n","            padding_idx (int): 패딩 인덱스\n","        \"\"\"\n","        super(NewsClassifier, self).__init__()\n","\n","        if pretrained_embeddings is None:\n","\n","            self.emb = nn.Embedding(embedding_dim=embedding_size,\n","                                    num_embeddings=num_embeddings,\n","                                    padding_idx=padding_idx)        \n","        else:\n","            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n","            self.emb = nn.Embedding(embedding_dim=embedding_size,\n","                                    num_embeddings=num_embeddings,\n","                                    padding_idx=padding_idx,\n","                                    _weight=pretrained_embeddings)\n","        \n","            \n","        self.convnet = nn.Sequential(\n","            nn.Conv1d(in_channels=embedding_size, \n","                   out_channels=num_channels, kernel_size=3),\n","            nn.ELU(),\n","            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n","                   kernel_size=3, stride=2),\n","            nn.ELU(),\n","            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n","                   kernel_size=3, stride=2),\n","            nn.ELU(),\n","            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n","                   kernel_size=3),\n","            nn.ELU()\n","        )\n","\n","        self._dropout_p = dropout_p\n","        self.fc1 = nn.Linear(num_channels, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x_in, apply_softmax=False):\n","        \"\"\"분류기의 정방향 계산\n","        \n","        매개변수:\n","            x_in (torch.Tensor): 입력 데이터 텐서 \n","                x_in.shape는 (batch, dataset._max_seq_length)입니다.\n","            apply_softmax (bool): 소프트맥스 활성화 함수를 위한 플래그\n","                크로스-엔트로피 손실을 사용하려면 False로 지정합니다\n","        반환값:\n","            결과 텐서. tensor.shape은 (batch, num_classes)입니다.\n","        \"\"\"\n","        \n","        # 임베딩을 적용하고 특성과 채널 차원을 바꿉니다\n","        x_embedded = self.emb(x_in).permute(0, 2, 1)\n","\n","        features = self.convnet(x_embedded)\n","\n","        # 평균 값을 계산하여 부가적인 차원을 제거합니다\n","        remaining_size = features.size(dim=2)\n","        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n","        features = F.dropout(features, p=self._dropout_p)\n","        \n","        # MLP 분류기\n","        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n","        prediction_vector = self.fc2(intermediate_vector)\n","\n","        if apply_softmax:\n","            prediction_vector = F.softmax(prediction_vector, dim=1)\n","\n","        return prediction_vector"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Py85Dwt_Bo1l","executionInfo":{"status":"ok","timestamp":1632826215476,"user_tz":-540,"elapsed":428,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["def make_train_state(args):\n","    return {'stop_early': False,\n","            'early_stopping_step': 0,\n","            'early_stopping_best_val': 1e8,\n","            'learning_rate': args.learning_rate,\n","            'epoch_index': 0,\n","            'train_loss': [],\n","            'train_acc': [],\n","            'val_loss': [],\n","            'val_acc': [],\n","            'test_loss': -1,\n","            'test_acc': -1,\n","            'model_filename': args.model_state_file}\n","\n","def update_train_state(args, model, train_state):\n","    \"\"\"훈련 상태를 업데이트합니다.\n","\n","    Components:\n","     - 조기 종료: 과대 적합 방지\n","     - 모델 체크포인트: 더 나은 모델을 저장합니다\n","\n","    :param args: 메인 매개변수\n","    :param model: 훈련할 모델\n","    :param train_state: 훈련 상태를 담은 딕셔너리\n","    :returns:\n","        새로운 훈련 상태\n","    \"\"\"\n","\n","    # 적어도 한 번 모델을 저장합니다\n","    if train_state['epoch_index'] == 0:\n","        torch.save(model.state_dict(), train_state['model_filename'])\n","        train_state['stop_early'] = False\n","\n","    # 성능이 향상되면 모델을 저장합니다\n","    elif train_state['epoch_index'] >= 1:\n","        loss_tm1, loss_t = train_state['val_loss'][-2:]\n","\n","        # 손실이 나빠지면\n","        if loss_t >= train_state['early_stopping_best_val']:\n","            # 조기 종료 단계 업데이트\n","            train_state['early_stopping_step'] += 1\n","        # 손실이 감소하면\n","        else:\n","            # 최상의 모델 저장\n","            if loss_t < train_state['early_stopping_best_val']:\n","                torch.save(model.state_dict(), train_state['model_filename'])\n","\n","            # 조기 종료 단계 재설정\n","            train_state['early_stopping_step'] = 0\n","\n","        # 조기 종료 여부 확인\n","        train_state['stop_early'] = \\\n","            train_state['early_stopping_step'] >= args.early_stopping_criteria\n","\n","    return train_state\n","\n","def compute_accuracy(y_pred, y_target):\n","    _, y_pred_indices = y_pred.max(dim=1)\n","    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","    return n_correct / len(y_pred_indices) * 100"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qf0OVgfIBse0","executionInfo":{"status":"ok","timestamp":1632826225260,"user_tz":-540,"elapsed":399,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["def set_seed_everywhere(seed, cuda):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if cuda:\n","        torch.cuda.manual_seed_all(seed)\n","\n","def handle_dirs(dirpath):\n","    if not os.path.exists(dirpath):\n","        os.makedirs(dirpath)\n","        \n","def load_glove_from_file(glove_filepath):\n","    \"\"\"GloVe 임베딩 로드 \n","    \n","    매개변수:\n","        glove_filepath (str): 임베딩 파일 경로 \n","    반환값:\n","        word_to_index (dict), embeddings (numpy.ndarary)\n","    \"\"\"\n","\n","    word_to_index = {}\n","    embeddings = []\n","    with open(glove_filepath, \"r\") as fp:\n","        for index, line in enumerate(fp):\n","            line = line.split(\" \") # each line: word num1 num2 ...\n","            word_to_index[line[0]] = index # word = line[0] \n","            embedding_i = np.array([float(val) for val in line[1:]])\n","            embeddings.append(embedding_i)\n","    return word_to_index, np.stack(embeddings)\n","\n","def make_embedding_matrix(glove_filepath, words):\n","    \"\"\"\n","    특정 단어 집합에 대한 임베딩 행렬을 만듭니다.\n","    \n","    매개변수:\n","        glove_filepath (str): 임베딩 파일 경로\n","        words (list): 단어 리스트\n","    \"\"\"\n","    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n","    embedding_size = glove_embeddings.shape[1]\n","    \n","    final_embeddings = np.zeros((len(words), embedding_size))\n","\n","    for i, word in enumerate(words):\n","        if word in word_to_idx:\n","            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n","        else:\n","            embedding_i = torch.ones(1, embedding_size)\n","            torch.nn.init.xavier_uniform_(embedding_i)\n","            final_embeddings[i, :] = embedding_i\n","\n","    return final_embeddings"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YgnKcT4_Bu7c","executionInfo":{"status":"ok","timestamp":1632826258002,"user_tz":-540,"elapsed":375,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"1458a382-303c-4a4e-ae2f-094172aea4c5"},"source":["##########설정 및 전처리\n","\n","from argparse import Namespace\n","args = Namespace(\n","    # 날짜와 경로 정보\n","    news_csv=\"data/ag_news/news_with_splits.csv\",\n","    vectorizer_file=\"vectorizer.json\",\n","    model_state_file=\"model.pth\",\n","    save_dir=\"model_storage/ch5/document_classification\",\n","    # 모델 하이퍼파라미터\n","    glove_filepath='data/glove/glove.6B.100d.txt', \n","    use_glove=False,\n","    embedding_size=100, \n","    hidden_dim=100, \n","    num_channels=100, \n","    # 훈련 하이퍼파라미터\n","    seed=1337, \n","    learning_rate=0.001, \n","    dropout_p=0.1, \n","    batch_size=128, \n","    num_epochs=100, \n","    early_stopping_criteria=5, \n","    # 실행 옵션\n","    cuda=True, \n","    catch_keyboard_interrupt=True, \n","    reload_from_files=False,\n","    expand_filepaths_to_save_dir=True\n",") \n","\n","if args.expand_filepaths_to_save_dir:\n","    args.vectorizer_file = os.path.join(args.save_dir,\n","                                        args.vectorizer_file)\n","\n","    args.model_state_file = os.path.join(args.save_dir,\n","                                         args.model_state_file)\n","    \n","    print(\"파일 경로: \")\n","    print(\"\\t{}\".format(args.vectorizer_file))\n","    print(\"\\t{}\".format(args.model_state_file))\n","    \n","# CUDA 체크\n","if not torch.cuda.is_available():\n","    args.cuda = False\n","    \n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"CUDA 사용여부: {}\".format(args.cuda))\n","\n","# 재현성을 위해 시드 설정\n","set_seed_everywhere(args.seed, args.cuda)\n","\n","# 디렉토리 처리\n","handle_dirs(args.save_dir)\n"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["파일 경로: \n","\tmodel_storage/ch5/document_classification/vectorizer.json\n","\tmodel_storage/ch5/document_classification/model.pth\n","CUDA 사용여부: False\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":474},"id":"r9teyi6XB_V1","executionInfo":{"status":"error","timestamp":1632826303782,"user_tz":-540,"elapsed":466,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"b42a86e7-dbf8-49b3-eb65-80fa0d9d1a0b"},"source":["args.use_glove = True\n","\n","if args.reload_from_files:\n","    # 체크포인트를 로드합니다.\n","    dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv,\n","                                                              args.vectorizer_file)\n","else:\n","    # 데이터셋과 Vectorizer를 만듭니다.\n","    dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n","    dataset.save_vectorizer(args.vectorizer_file)\n","vectorizer = dataset.get_vectorizer()\n","\n","# GloVe를 사용하거나 랜덤하게 임베딩을 초기화합니다\n","if args.use_glove:\n","    words = vectorizer.title_vocab._token_to_idx.keys()\n","    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n","                                       words=words)\n","    print(\"사전 훈련된 임베딩을 사용합니다\")\n","else:\n","    print(\"사전 훈련된 임베딩을 사용하지 않습니다\")\n","    embeddings = None\n","\n","classifier = NewsClassifier(embedding_size=args.embedding_size, \n","                            num_embeddings=len(vectorizer.title_vocab),\n","                            num_channels=args.num_channels,\n","                            hidden_dim=args.hidden_dim, \n","                            num_classes=len(vectorizer.category_vocab), \n","                            dropout_p=args.dropout_p,\n","                            pretrained_embeddings=embeddings,\n","                            padding_idx=0)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-327f37dc9e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 데이터셋과 Vectorizer를 만듭니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewsDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_and_make_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-a4b0a9fc6692>\u001b[0m in \u001b[0;36mload_dataset_and_make_vectorizer\u001b[0;34m(cls, news_csv)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#새로운 vectorizer 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset_and_make_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mnews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_news_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNewsVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_news_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ag_news/news_with_splits.csv'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"TJqvcCpjB27W","executionInfo":{"status":"error","timestamp":1632826276669,"user_tz":-540,"elapsed":386,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}},"outputId":"224455bc-031d-43fc-8fc4-20dff80dcc0e"},"source":["########훈련 루프\n","\n","classifier = classifier.to(args.device)\n","dataset.class_weights = dataset.class_weights.to(args.device)\n","    \n","loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n","optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n","                                           mode='min', factor=0.5,\n","                                           patience=1)\n","\n","train_state = make_train_state(args)\n","\n","epoch_bar = tqdm_notebook(desc='training routine', \n","                          total=args.num_epochs,\n","                          position=0)\n","\n","dataset.set_split('train')\n","train_bar = tqdm_notebook(desc='split=train',\n","                          total=dataset.get_num_batches(args.batch_size), \n","                          position=1, \n","                          leave=True)\n","dataset.set_split('val')\n","val_bar = tqdm_notebook(desc='split=val',\n","                        total=dataset.get_num_batches(args.batch_size), \n","                        position=1, \n","                        leave=True)\n","\n","try:\n","    for epoch_index in range(args.num_epochs):\n","        train_state['epoch_index'] = epoch_index\n","\n","        # 훈련 세트에 대한 순회\n","\n","        # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n","        dataset.set_split('train')\n","        batch_generator = generate_batches(dataset, \n","                                           batch_size=args.batch_size, \n","                                           device=args.device)\n","        running_loss = 0.0\n","        running_acc = 0.0\n","        classifier.train()\n","\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","            # 훈련 과정은 5단계로 이루어집니다\n","\n","            # --------------------------------------\n","            # 단계 1. 그레이디언트를 0으로 초기화합니다\n","            optimizer.zero_grad()\n","\n","            # 단계 2. 출력을 계산합니다\n","            y_pred = classifier(batch_dict['x_data'])\n","\n","            # 단계 3. 손실을 계산합니다\n","            loss = loss_func(y_pred, batch_dict['y_target'])\n","            loss_t = loss.item()\n","            running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n","            loss.backward()\n","\n","            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n","            optimizer.step()\n","            # -----------------------------------------\n","            \n","            # 정확도를 계산합니다\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","            # 진행 상태 막대 업데이트\n","            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n","                                  epoch=epoch_index)\n","            train_bar.update()\n","\n","        train_state['train_loss'].append(running_loss)\n","        train_state['train_acc'].append(running_acc)\n","\n","        # 검증 세트에 대한 순회\n","\n","        # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n","        dataset.set_split('val')\n","        batch_generator = generate_batches(dataset, \n","                                           batch_size=args.batch_size, \n","                                           device=args.device)\n","        running_loss = 0.\n","        running_acc = 0.\n","        classifier.eval()\n","\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","\n","            # 단계 1. 출력을 계산합니다\n","            y_pred =  classifier(batch_dict['x_data'])\n","\n","            # 단계 2. 손실을 계산합니다\n","            loss = loss_func(y_pred, batch_dict['y_target'])\n","            loss_t = loss.item()\n","            running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","            # 단계 3. 정확도를 계산합니다\n","            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n","            running_acc += (acc_t - running_acc) / (batch_index + 1)\n","            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n","                            epoch=epoch_index)\n","            val_bar.update()\n","\n","        train_state['val_loss'].append(running_loss)\n","        train_state['val_acc'].append(running_acc)\n","\n","        train_state = update_train_state(args=args, model=classifier,\n","                                         train_state=train_state)\n","\n","        scheduler.step(train_state['val_loss'][-1])\n","\n","        if train_state['stop_early']:\n","            break\n","\n","        train_bar.n = 0\n","        val_bar.n = 0\n","        epoch_bar.update()\n","except KeyboardInterrupt:\n","    print(\"Exiting loop\")"],"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-31e34dd357e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m########훈련 루프\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"]}]},{"cell_type":"code","metadata":{"id":"MSU6QWZ1B7eM","executionInfo":{"status":"ok","timestamp":1632826371502,"user_tz":-540,"elapsed":375,"user":{"displayName":"Jean Kim","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11015366145627632574"}}},"source":["###################inference\n","\n","# 리뷰 텍스트를 전처리\n","def preprocess_text(text):\n","    text = ' '.join(word.lower() for word in text.split(\" \"))\n","    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n","    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n","    return text\n","\n","def predict_category(title, classifier, vectorizer, max_length):\n","    \"\"\"뉴스 제목을 기반으로 카테고리를 예측합니다\n","    \n","    매개변수:\n","        title (str): 원시 제목 문자열\n","        classifier (NewsClassifier): 훈련된 분류기 객체\n","        vectorizer (NewsVectorizer): 해당 Vectorizer\n","        max_length (int): 최대 시퀀스 길이\n","            노트: CNN은 입력 텐서 크기에 민감합니다. \n","                 훈련 데이터처럼 동일한 크기를 갖도록 만듭니다.\n","    \"\"\"\n","    title = preprocess_text(title)\n","    vectorized_title = \\\n","        torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n","    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n","    probability_values, indices = result.max(dim=1)\n","    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n","\n","    return {'category': predicted_category, \n","            'probability': probability_values.item()}\n","\n","def get_samples():\n","    samples = {}\n","    for cat in dataset.val_df.category.unique():\n","        samples[cat] = dataset.val_df.title[dataset.val_df.category==cat].tolist()[:5]\n","    return samples\n","\n","val_samples = get_samples()\n","\n","\n","classifier = classifier.to(\"cpu\")\n","\n","for truth, sample_group in val_samples.items():\n","    print(f\"True Category: {truth}\")\n","    print(\"=\"*30)\n","    for sample in sample_group:\n","        prediction = predict_category(sample, classifier, \n","                                      vectorizer, dataset._max_seq_length + 1)\n","        print(\"예측: {} (p={:0.2f})\".format(prediction['category'],\n","                                                  prediction['probability']))\n","        print(\"\\t + 샘플: {}\".format(sample))\n","    print(\"-\"*30 + \"\\n\")\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"4WN-6Y6uCQD8"},"source":[""],"execution_count":null,"outputs":[]}]}